hidden_layer_sizes: [256, 128, 64]   # taille qui tourne bien chez moi (stable + rapide)
learning_rate_init: 0.0005           # je reste prudent sur le LR
alpha: 0.001                          # un poil de régul L2
batch_size: 2048                      # 4096 me donnait des pertes instables
max_iter: 100                         # je laisse le modèle itérer un peu
early_stopping: True
n_iter_no_change: 15                  # je laisse 15 iters pour plateaux
validation_fraction: 0.15             # split val un peu plus large
tol: 1e-4
